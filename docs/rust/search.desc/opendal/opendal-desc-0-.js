searchState.loadedDescShard("opendal", 0, "Apache OpenDAL™ is an Open Data Access Layer that …\naliyun_drive: Aliyun Drive services.\nalluxio: Alluxio services.\nThe given path already exists thus we failed to the …\n[atomicserver][crate::services::Atomicserver]: …\nazblob: Azure Storage Blob services.\nAzdls: Azure Data Lake Storage Gen2.\nazfile: Azfile Services\nB2: Backblaze B2 Services.\nBuffer is a wrapper of contiguous <code>Bytes</code> and non-contiguous …\nBufferSink is the adapter of <code>futures::Sink</code> generated by …\nBufferStream is a stream of buffers, created by …\nBuilder is used to set up underlying services.\nAssociated builder for this configuration.\ncacache: cacache backend support.\nCapability defines the supported operations and their …\ncloudflare-kv: Cloudflare KV services.\nCompfs: Compio fs Services.\nThe condition of this operation is not match.\nAssociated configuration for this builder.\nThe config for backend is invalid.\nConfigurator is used to configure the underlying service.\ncos: Tencent Cloud Object Storage services.\nCustom that allow users to implement services outside …\nd1: D1 services\nDIR means the path can be listed.\ndashmap: dashmap backend support.\ndbfs: DBFS backend support.\nDeleteInput is the input for delete operations.\nDeleter is designed to continuously remove content from …\ndropbox: Dropbox services.\nEntry returned by <code>Lister</code> or [<code>BlockingLister</code>] to represent …\nEntryMode represents the mode.\nContains the error value\nError is the error struct returned by all opendal …\nErrorKind is all kinds of Error of opendal.\netcd: Etcd Services\nExecute trait is used to execute task in background.\nExecutor that runs futures in background.\nFILE means the path has data to read.\nfoundationdb: Foundationdb services.\nfs: POSIX-like file system.\nftp: FTP backend.\nFuturesAsyncReader is the adapter of <code>AsyncRead</code>, …\nFuturesIoAsyncWriter is the adapter of <code>AsyncWrite</code> for …\nFuturesBytesSink is the adapter of <code>futures::Sink</code> generated …\nFuturesBytesStream is the adapter of <code>Stream</code> generated by …\nFuturesDeleteSink is a sink that generated by <code>Deleter</code>\ngcs: Google Cloud Storage backend.\ngdrive: GoogleDrive services.\nghac: GitHub Action Cache services.\nGithub Contents: Github contents support.\ngridfs: MongoDB Gridfs Services\nhdfs: Hadoop Distributed File System.\nNative HDFS: Hdfs Native service, using rust hdfs-native …\nhttp: HTTP backend.\nhuggingface: Huggingface services.\n[icloud][crate::services::Icloud]: APPLE icloud services.\nIntoDeleteInput is a helper trait that makes it easier for …\nipmfs: IPFS HTTP Gateway\nipmfs: IPFS mutable file system\nThe given path is a directory.\nThe given file paths are same.\nKoofr: Koofr Services.\nlakefs: LakeFS Services\nLister is designed to list entries at given path in an …\nmemcached: Memcached service support.\nmemory: In memory backend support.\nMetadata contains all the information related to a …\nmini-moka: Mini Moka backend support.\nmoka: moka backend support.\nmongodb: MongoDB Services\nmonoiofs: monoio fs services.\nmysql: Mysql services\nNebulaGraph: NebulaGraph Services\nThe given path is not a directory.\nThe given path is not found.\nobs: Huawei Cloud OBS services.\nContains the success value\nonedrive: Microsoft OneDrive services.\nThe <code>Operator</code> serves as the entry point for all public …\nOperatorBuilder is a typed builder to build an Operator.\nMetadata for operator, users can use this metadata to get …\noss: Aliyun Object Storage Services\nPcloud: Pcloud Services.\nThe given path doesn’t have enough permission for this …\npersy: persy backend support.\npostgresql: Postgresql services\nThe range of the content is not satisfied.\nRequests that sent to this path is over the limit, please …\nReader is designed to read data from given path in an …\nredb: Redb Services\nredis: Redis services\nResult that is a wrapper of <code>Result&lt;T, opendal::Error&gt;</code>\nrocksdb: RocksDB services\ns3: AWS S3 alike services.\nAssociated scheme for this builder. It indicates what …\nServices that OpenDAL supports\nSeafile: Seafile Services.\nsftp: SFTP services\nsled: Sled services\nsqlite: Sqlite services\nsurrealdb: Surrealdb Services\nswift: Swift backend support.\ntikv: Tikv Services\nOpenDAL don’t know what happened here, and no actions …\nUnknown means we don’t know what we can do on this path.\nUnderlying service doesn’t support this operation.\nUpyun: Upyun Services.\nVercel Artifacts: Vercel Artifacts service, as known as …\nVercelBlob: VercelBlob Services.\nwebdav: WebDAV support.\nwebhdfs: WebHDFS RESTful API Services\nWriter is designed to write data into given path in an …\nYandexDisk: YandexDisk Services.\nAbort the writer and clean up all written data.\nReturn error’s backtrace.\nblocking module provides blocking APIs for OpenDAL.\nConsume the accessor builder to build a service.\nCache control of this entry.\nCheck if this operator can work correctly.\nClose the writer and make sure all data have been …\nClose the deleter, this will flush the deleter and wait …\nContent-Disposition of this entry\nContent Encoding of this entry.\nContent length of this entry.\nContent MD5 of this entry.\nContent Range of this entry.\nContent Type of this entry.\nCopy a file from <code>from</code> to <code>to</code>.\nIndicates if copy operations are supported.\nNumber of <code>Bytes</code> in <code>Buffer</code>.\nCreate a directory at the specified path.\nIndicates if directory creation is supported.\nGet current <code>Bytes</code>.\nDelete a path.\nDelete the given path.\nIndicates if delete operations are supported.\nDelete an infallible iterator of paths.\nDelete an infallible iterator of paths.\nMaximum size supported for single delete operations.\nDelete the given path with additional options.\nDelete an infallible stream of paths.\nDelete an infallible stream of paths.\nDelete a fallible iterator of paths.\nDelete a fallible iterator of paths.\nDelete a fallible stream of paths.\nDelete a fallible stream of paths.\nDelete the given path with additional options.\nIndicates if versions delete operations are supported.\nCreate a <code>Deleter</code> to continuously remove content from …\nThis module holds documentation for OpenDAL.\nGet all enabled schemes.\nETag of this entry.\nExecute async task in background.\nGet the executor used by current operator.\nexecutors module provides implementations for the <code>Execute</code> …\nCheck whether this path exists.\nFetch specific ranges from reader.\nFinish the building to construct an Operator.\nFlush the deleter, returns the number of deleted paths.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreate a new operator from given config.\nConvert inner accessor into operator.\nDeserialize from an iterator.\nDeserialize from an iterator.\nCreate a new operator from given iterator in static …\nCreate a new operator from given map.\nGet [<code>Full Capability</code>] of operator.\nGet the http client used by current operator.\nGet information of underlying accessor.\nFetch the internal accessor.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConvert this configuration into a service builder.\nConvert writer into <code>FuturesBytesSink</code> which implements …\nConvert reader into <code>FuturesBytesStream</code> which implements …\nConvert <code>self</code> into a <code>DeleteInput</code>.\nConvert reader into <code>FuturesAsyncReader</code> which implements …\nConvert writer into <code>FuturesAsyncWriter</code> which implements …\nConvert operator into inner accessor.\nConsume this entry to get its path and metadata.\nConvert writer into <code>BufferSink</code> which implements […\nConvert the deleter into a sink.\nConvert self into static str.\nConvert self into static str.\nCreate a buffer stream to read specific range from given …\nChecks whether the metadata corresponds to the most recent …\nChecks if the file (or version) associated with this …\nCheck if this mode is DIR.\nReturns <code>true</code> if this metadata is for a directory.\nCheck if buffer is empty.\nCheck if this mode is FILE.\nReturns <code>true</code> if this metadata is for a file.\nCheck if this error is temporary.\nReturn error’s kind.\nLast modified of this entry.\nCreate a new layer with static dispatch.\nCreate a new layer with dynamic dispatch.\n<code>Layer</code> is the mechanism to intercept operations.\nGet the length of the buffer.\nList entries in the parent directory that start with the …\nIndicates if list operations are supported.\nList entries in the parent directory that start with the …\nList entries in the parent directory that start with the …\nIndicates if listing with deleted files included is …\nIndicates if list operations support result limiting.\nIndicates if recursive listing is supported.\nIndicates if list operations support continuation from a …\nIndicates if versions listing is supported.\nIndicates if listing with versions included is supported.\nCreate a new lister to list entries that starts with given …\nCreate a new lister to list entries that starts with given …\nCreate a new lister to list entries that starts with given …\nOperate on error with map.\nFetch metadata of this entry.\nmode represent this entry’s mode.\nName of entry. Name is the last segment of path.\nName of backend, could be empty if underlying backend doesn…\nGet [<code>Native Capability</code>] of operator.\nCreate a new operator builder.\nCreate a new Error with error kind and message.\nCreate a new empty buffer.\nCreate a new metadata\nCreate a default executor.\nCreate a new operator with input builder.\nFutures provides the futures generated by <code>Operator</code>\nOptions module provides options definitions for operations.\nPath of entry. Path is relative to operator’s root.\nThe path of the path to delete.\nIndicates if presigned URL generation is supported.\nPresign an operation for delete.\nIndicates if presigned URLs for delete operations are …\nPresign an operation for delete with additional options.\nPresign an operation for delete without extra options.\nPresign an operation for read.\nIndicates if presigned URLs for read operations are …\nPresign an operation for read with additional options.\nPresign an operation for read with extra options.\nPresign an operation for stat(head).\nIndicates if presigned URLs for stat operations are …\nPresign an operation for stat(head) with additional …\nPresign an operation for stat(head).\nPresign an operation for write.\nIndicates if presigned URLs for write operations are …\nPresign an operation for write with additional options.\nPresign an operation for write with extra options.\nRaw modules provide raw APIs that used by underlying …\nRead give range from reader into <code>Buffer</code>.\nRead the entire file into bytes from given path.\nIndicates if the operator supports read operations.\nRead all data from reader into given <code>BufMut</code>.\nRead the entire file into bytes from given path with …\nRead all bytes until EOF.\nRead the entire file into bytes from given path with …\nIndicates if conditional read operations using If-Match …\nIndicates if conditional read operations using …\nIndicates if conditional read operations using …\nIndicates if conditional read operations using …\nIndicates if Cache-Control header override is supported …\nIndicates if Content-Disposition header override is …\nIndicates if Content-Type header override is supported …\nIndicates if versions read operations are supported.\nCreate a new reader of given path.\nCreate a new reader of given path with additional options.\nCreate a new reader of given path with additional options.\nRemove the path and all nested dirs and files recursively.\nRename a file from <code>from</code> to <code>to</code>.\nIndicates if rename operations are supported.\nRoot of operator, will be in format like <code>/path/to/dir/</code>\n<code>Scheme</code> of operator.\nServices will provide builders to build underlying …\nSet cache control of this entry.\nSet Content-Disposition of this entry\nSet Content Encoding of this entry.\nSet content length of this entry.\nSet content MD5 of this entry.\nSet Content Range of this entry.\nSet Content Type of this entry.\nSet ETag of this entry.\nSet the <code>is_current</code> status of this entry.\nSet the deleted status of this entry.\nSet Last modified of this entry.\nSet mode for entry.\nSet permanent status for error.\nSet persistent status for error.\nSet source for error.\nSet temporary status for error.\nSet the version of the file\nIndicate if the operator supports shared access.\nReturns a slice of self for the provided range.\nRetrieve the metadata for the specified path.\nIndicates if the operator supports metadata retrieval …\nRetrieve the metadata of the specified path with …\nRetrieve the metadata of the specified path with …\nIndicates if conditional stat operations using If-Match …\nIndicates if conditional stat operations using …\nIndicates if conditional stat operations using …\nIndicates if conditional stat operations using …\nIndicates if Cache-Control header override is supported …\nIndicates if Content-Disposition header override is …\nIndicates if Content-Type header override is supported …\nIndicates if versions stat operations are supported.\nReturn a future that will be resolved after the given …\nReturn a future that will be resolved after the given …\nCombine all bytes together into one single <code>Bytes</code>.\nConvert buffer into a slice of <code>IoSlice</code> for vectored write.\nCombine all bytes together into one single <code>Vec&lt;u8&gt;</code>.\nShortens the buffer, keeping the first <code>len</code> bytes and …\nUpdate executor for the context.\nUpdate http client for the context.\nUser defined metadata of this entry\nRetrieves the <code>version</code> of the file, if available.\nThe version of the path to delete.\nCreate a new operator via given scheme and iterator of …\nCreate a new operator from given scheme and map.\nCreate a new executor with given execute impl.\nSet cache control of this entry.\nSet Content-Disposition of this entry\nSet content length of this entry.\nSet content MD5 of this entry.\nSet Content Range of this entry.\nSet Content Type of this entry.\nAdd more context in error.\nSet ETag of this entry.\nSet the <code>is_current</code> status of this entry.\nSet the deleted status of this entry.\nSet Last modified of this entry.\nSet mode for entry.\nUpdate error’s operation.\nWith user defined metadata of this entry\nWith the version of the file.\nWrite <code>Buffer</code> into writer.\nWrite all data to the specified path at once.\nIndicates if the operator supports write operations.\nIndicates if append operations are supported.\nIndicates if writing empty content is supported.\nIndicates if multiple write operations can be performed on …\nWrite <code>bytes::Buf</code> into inner writer.\nMaximum size supported for multipart uploads. For example, …\nMinimum size required for multipart uploads (except for …\nWrite all data to the specified path at once with …\nMaximum total size supported for write operations. For …\nWrite all data to the specified path at once with …\nIndicates if Cache-Control can be specified during write …\nIndicates if Content-Disposition can be specified during …\nIndicates if Content-Encoding can be specified during …\nIndicates if Content-Type can be specified during write …\nIndicates if conditional write operations using If-Match …\nIndicates if conditional write operations using …\nIndicates if write operations can be conditional on object …\nIndicates if custom user metadata can be attached during …\nCreate a new writer of given path.\nCreate a new writer of given path with additional options.\nCreate a new writer of given path with additional options.\nBufferIterator is an iterator of buffers.\nBlockingDeleter is designed to continuously remove content …\nBlockingLister is designed to list entries at given path …\nUse OpenDAL in blocking context.\nBlockingReader is designed to read data from given path in …\nStdIterator is the adapter of <code>Iterator</code> for [<code>BlockingReader</code>]…\nStdReader is the adapter of <code>Read</code>, <code>Seek</code> and <code>BufRead</code> for […\nStdWriter is the adapter of <code>std::io::Write</code> for […\nBlockingWriter is designed to write data into given path …\nCheck if this operator can work correctly.\nClose the deleter, this will flush the deleter and wait …\nClose the writer and make sure all data have been …\nClose the internal writer and make sure all data have been …\nCopy a file from <code>from</code> to <code>to</code>.\nCreate a dir at given path.\nDelete a path.\nDelete given path.\nDelete an infallible iterator of paths.\nDelete an infallible iterator of paths.\nDelete given path with options.\nDelete an fallible iterator of paths.\nDelete a fallible iterator of paths.\nCreate a [<code>BlockingDeleter</code>] to continuously remove content …\nCheck if this path exists or not.\nFlush the deleter, returns the number of deleted paths.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet information of underlying accessor.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConvert reader into <code>StdBytesIterator</code> which implements …\nCreate a buffer iterator to read specific range from given …\nConvert reader into <code>StdReader</code> which implements …\nConvert writer into <code>StdWriter</code> which implements …\nList entries that starts with given <code>path</code> in parent dir.\nList entries that starts with given <code>path</code> in parent dir. …\nList entries that starts with given <code>path</code> in parent dir.\nList entries within a given directory as an iterator with …\nCreate a new <code>BlockingLayer</code> with the current runtime’s …\nRead give range from reader into <code>Buffer</code>.\nRead the whole path into a bytes.\nThis operation will copy and write bytes into given <code>BufMut</code>…\nRead the whole path into a bytes with extra options.\nRead all bytes until EOF.\nCreate a new reader which can read the whole path.\nCreate a new reader with extra options\nRemove the path and all nested dirs and files recursively.\nRename a file from <code>from</code> to <code>to</code>.\nGet given path’s metadata.\nGet given path’s metadata with extra options.\nWrite <code>Buffer</code> into writer.\nWrite bytes into given path.\nWrite data with options.\nWrite multiple bytes into given path.\nCreate a new writer with extra options\nChanges log for all OpenDAL released versions.\nCompare opendal with other projects to find out the …\nThe core concepts of OpenDAL’s public API.\nThe internal implement details of OpenDAL.\nOpenDAL Performance Guide\nRFCs - OpenDAL Active RFC List\nUpgrade and migrate procedures while OpenDAL meets …\nOpenDAL vs object_store\nThe internal implementation details of <code>Access</code>.\nThe internal implementation details of <code>Layer</code>.\nConcurrent Write\nHTTP Optimization\nRFC example\nObject native API\nError handle\nAuto region\nObject stream\nLimited reader\nPath normalization\nAsync streaming IO\nRemove credential\nCreate dir\nRetryable error\nObject ID\nDir entry\nAccessor capabilities\nPresign\nCommand line interface\nInit from iter\nMultipart\nGateway\nNew builder\nWrite refactor\nList metadata reuse\nBlocking API\nRedis service\nSplit capabilities\nPath in accessor\nGeneric KV services\nObject reader\nRefactor error\nObject handler\nObject metadataer\nQuery based metadata\nObject writer\nRemove object concept\nOperation extension\nWriter sink API\nAppend API\nChain based operator API\nObject versioning\nMerge append into write\nLister API\nList with metakey\nNative capability\nRemove write copy from\nConfig\nAlign list API\nList prefix\nLazy reader\nList recursive\nConcurrent stat in list\nBuffered Reader\nConcurrent Writer\nDeleter API\nRange Based Read API\nExecutor API\nRemove metakey\nOperator from uri\nContext\nConditional Reader\nList With Deleted\nWrite Returns Metadata\nRead Returns Metadata\nRemove Native Blocking\nGlob support\nOptions API\nExecutor that uses the <code>tokio::task::spawn</code> to execute …\nTokio’s JoinHandle has its own <code>abort</code> support, so …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nAdd Efficient, logical ‘stack’ traces of async …\nAdd an Instrument await-tree for actor-based applications …\nAdd an extra capability check layer for every operation\nInject chaos into underlying services for robustness test.\nAdd concurrent request limit.\nSupport User Statically-Defined Tracing(aka USDT) on Linux\nAdd fastmetrics for every operation.\n<code>FastmetricsLayerBuilder</code> is a config builder to build a …\nAdd fastrace for every operation.\nLayer for replacing the default HTTP client with a custom …\nAdd an immutable in-memory index for underlying storage …\nLoggingInterceptor is used to intercept the log.\nAdd log for every operation.\nAdd metrics for every operation.\nA layer that can automatically set <code>Content-Type</code> based on …\nAdd opentelemetry::metrics for every operation.\nAdd opentelemetry::trace for every operation.\nAdd prometheus-client for every operation.\n<code>PrometheusClientLayerBuilder</code> is a config builder to build …\nAdd prometheus for every operation.\n<code>PrometheusLayerBuilder</code> is a config builder to build a …\nRetryInterceptor is used to intercept while retry happened.\nAdd retry for temporary failed operations.\nAdd a bandwidth rate limiter to the underlying services.\nAdd timeout for every operation to avoid slow or …\nAdd tracing for every operation.\nCreate a <code>PrometheusLayerBuilder</code> to set the configuration …\nCreate a <code>PrometheusClientLayerBuilder</code> to set the …\nCreate a <code>FastmetricsLayerBuilder</code> to set the configuration …\nCreate a <code>OtelMetricsLayerBuilder</code> to set the configuration …\nSet buckets for bytes related histogram like …\nSet buckets for bytes related histogram like …\nSet buckets for bytes related histogram like …\nSet buckets for bytes rate related histogram like …\nSet buckets for bytes rate related histogram like …\nSet buckets for bytes rate related histogram like …\nThe ‘root’ label might have risks of being high …\nThe ‘root’ label might have risks of being high …\nSet buckets for duration seconds related histogram like …\nSet buckets for duration seconds related histogram like …\nSet buckets for duration seconds related histogram like …\nSet buckets for entries related histogram like …\nSet buckets for entries related histogram like …\nSet buckets for entries related histogram like …\nSet buckets for entries rate related histogram like …\nSet buckets for entries rate related histogram like …\nSet buckets for entries rate related histogram like …\nInsert keys from iter.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nInsert a key into index.\nEverytime RetryLayer is retrying, this function will be …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nEverytime there is a log, this function will be called.\nCreate a new retry layer.\nCreate a new ConcurrentLimitLayer will specify permits.\nCreate the layer with specific logging interceptor.\nCreate a new <code>TimeoutLayer</code> with default settings.\nCreate a new chaos layer with specified error ratio.\nCreate a new <code>ThrottleLayer</code> with given bandwidth and burst.\nCreate a new <code>AwaitTreeLayer</code>.\nCreate a new <code>HttpClientLayer</code> with the given HTTP client.\nOpenDAL Observability\nRegister the metrics into the given registry and return a …\nRegister the metrics into the registry and return a …\nRegister the metrics into the registry and return a …\nRegister the metrics into the default registry and return …\nRegister the metrics into the global registry and return a …\nSet buckets for ttfb related histogram like …\nSet buckets for ttfb related histogram like …\nSet buckets for ttfb related histogram like …\nSet factor of current backoff.\nSet a concurrent limit for HTTP requests.\nSet io timeout for TimeoutLayer with given value.\nSet jitter of current backoff.\nSet max_delay of current backoff.\nSet max_times of current backoff.\nSet min_delay of current backoff.\nSet the retry interceptor as new notify.\nSet speed for TimeoutLayer with given value.\nSet timeout for TimeoutLayer with given value.\nBuckets for data size metrics like OperationBytes Covers …\nBuckets for data transfer rate metrics like …\nBuckets for operation duration metrics like …\nBuckets for batch operation entry counts …\nBuckets for batch operation processing rates …\nBuckets for time to first byte metrics like …\nIncrement the counter for HTTP connection errors. Metrics …\nUpdate the current number of executing HTTP requests. …\nRecord the size of HTTP request body in bytes. Metrics …\nRecord the rate of HTTP request data in bytes/second. …\nRecord the duration of sending an HTTP request (until …\nRecord the size of HTTP response body in bytes. Metrics …\nRecord the rate of HTTP response data in bytes/second. …\nRecord the duration of receiving an HTTP response (from …\nIncrement the counter for HTTP status errors (non-2xx …\nThe metric label for the error.\nThe metric label for the namespace like bucket name in s3.\nThe metric label for the operation like read, write, list.\nThe metric label for the root path.\nThe metric label for the scheme like s3, fs, cos.\nThe metric label for the http code.\nMetricLabels are the labels for the metrics.\nMetricValue is the value the opendal sends to the metrics …\nThe metrics accessor for opendal.\nThe interceptor for metrics.\nThe metrics layer for opendal.\nRecord the size of data processed in bytes. Metrics impl: …\nRecord the rate of data processing in bytes/second. …\nRecord the total duration of an operation. Metrics impl: …\nRecord the number of entries (files, objects, keys) …\nRecord the rate of entries processing in entries/second. …\nIncrement the counter for operation errors. Metrics impl: …\nUpdate the current number of executing operations. Metrics …\nRecord the time to first byte duration. Metrics impl: …\nThe specific error kind that occurred during an operation. …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the help text for this metric value.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns the full metric name for this metric value.\nReturns the metric name along with unit for this metric …\nThe storage namespace (e.g., bucket name, container name). …\nCreate a new metrics layer.\nObserve the metric value.\nObserve the metric value.\nThe operation being performed (e.g., “read”, “write…\nThe root path within the namespace that was configured. …\nThe storage scheme identifier (e.g., “s3”, “gcs”, …\nThe HTTP status code received in an error response. Only …\nFuture that generated by <code>Operator::delete_with</code>.\nFuture that generated by [<code>Operator::deleter_with</code>].\nFuture that generated by <code>Operator::list_with</code> or …\nFuture that generated by <code>Operator::list_with</code> or …\nFuture that generated by <code>Operator::presign_delete_with</code>.\nFuture that generated by <code>Operator::presign_read_with</code>.\nFuture that generated by <code>Operator::presign_stat_with</code>.\nFuture that generated by <code>Operator::presign_write_with</code>.\nFuture that generated by <code>Operator::read_with</code>.\nFuture that generated by <code>Operator::read_with</code> or …\nFuture that generated by <code>Operator::stat_with</code>.\nFuture that generated by <code>Operator::write_with</code>.\nFuture that generated by <code>Operator::writer_with</code>.\nOperatorFuture is the future generated by <code>Operator</code>.\nSets append mode for this write request.\nSets append mode for this write request.\nSets append mode for this write request.\nSets append mode for this write request.\nRefer to <code>options::WriteOptions::cache_control</code> for more …\nSets Cache-Control header for this write operation.\nSets Cache-Control header for this write operation.\nRefer to <code>options::WriteOptions::cache_control</code> for more …\nSets Cache-Control header for this write operation.\nSets Cache-Control header for this write operation.\nOpenDAL will use services’ preferred chunk size by …\nSets chunk size for buffered writes.\nSets chunk size for buffered writes.\nOpenDAL will use services’ preferred chunk size by …\nOpenDAL will use services’ preferred chunk size by …\nOpenDAL will use services’ preferred chunk size by …\nSets chunk size for buffered writes.\nSets chunk size for buffered writes.\nSet <code>concurrent</code> for the reader.\nSets concurrent write operations for this writer.\nSets concurrent write operations for this writer.\nSet <code>concurrent</code> for the reader.\nSet <code>concurrent</code> for the reader.\nSet <code>concurrent</code> for the reader.\nSets concurrent write operations for this writer.\nSets concurrent write operations for this writer.\nSets Content-Disposition header for this write request.\nRefer to <code>options::WriteOptions::content_disposition</code> for …\nSets Content-Disposition header for this write request.\nRefer to <code>options::WriteOptions::content_disposition</code> for …\nSets Content-Disposition header for this write request.\nSets Content-Disposition header for this write request.\nSets Content-Encoding header for this write request.\nSets Content-Encoding header for this write request.\nRefer to <code>options::WriteOptions::content_encoding</code> for more …\nRefer to <code>options::WriteOptions::content_encoding</code> for more …\nSets Content-Encoding header for this write request.\nSets Content-Encoding header for this write request.\nRefer to <code>options::WriteOptions::content_type</code> for more …\nSets <code>Content-Type</code> header for this write operation.\nSets <code>Content-Type</code> header for this write operation.\nRefer to <code>options::WriteOptions::content_type</code> for more …\nSets <code>Content-Type</code> header for this write operation.\nSets <code>Content-Type</code> header for this write operation.\nControls whether the <code>list</code> operation should include deleted …\nControls whether the <code>list</code> operation should include deleted …\nControls whether the <code>list</code> operation should include deleted …\nControls whether the <code>list</code> operation should include deleted …\nReturns the argument unchanged.\nControls the optimization strategy for range reads in …\nControls the optimization strategy for range reads in …\nSets If-Match header for this write request.\nSet the If-Match for this operation.\nSets If-Match header for this write request.\nSet <code>if_match</code> for this <code>read</code> request.\nRefer to <code>options::ReadOptions::if_match</code> for more details.\nSet <code>if-match</code> for this <code>read</code> request.\nRefer to <code>options::StatOptions::if_match</code> for more details.\nSet the If-Match for this operation.\nRefer to <code>options::StatOptions::if_match</code> for more details.\nRefer to <code>options::ReadOptions::if_match</code> for more details.\nSet <code>if_match</code> for this <code>read</code> request.\nSet <code>if-match</code> for this <code>read</code> request.\nSets If-Match header for this write request.\nSets If-Match header for this write request.\n<code>if_modified_since</code>\nSet the If-Modified-Since for this operation.\nSet <code>if-modified-since</code> for this <code>read</code> request.\nSet the If-Modified-Since for this operation.\n<code>if_modified_since</code>\nSet <code>if-modified-since</code> for this <code>read</code> request.\nRefer to <code>options::ReadOptions::if_none_match</code> for more …\nSets If-None-Match header for this write request.\nRefer to <code>options::StatOptions::if_none_match</code> for more …\nSets If-None-Match header for this write request.\nSet the If-None-Match for this operation.\nSet <code>if-none-match</code> for this <code>read</code> request.\nSet <code>if_none_match</code> for this <code>read</code> request.\nSet the If-None-Match for this operation.\nRefer to <code>options::StatOptions::if_none_match</code> for more …\nRefer to <code>options::ReadOptions::if_none_match</code> for more …\nSet <code>if_none_match</code> for this <code>read</code> request.\nSet <code>if-none-match</code> for this <code>read</code> request.\nSets If-None-Match header for this write request.\nSets If-None-Match header for this write request.\nSets the condition that write operation will succeed only …\nSets the condition that write operation will succeed only …\nSets the condition that write operation will succeed only …\nSets the condition that write operation will succeed only …\nSet the If-Unmodified-Since for this operation.\nSet <code>if_unmodified_since</code> for this <code>read</code> request.\nSet <code>if-unmodified-since</code> for this <code>read</code> request.\nSet the If-Unmodified-Since for this operation.\nSet <code>if_unmodified_since</code> for this <code>read</code> request.\nSet <code>if-unmodified-since</code> for this <code>read</code> request.\nCalls <code>U::from(self)</code>.\nThe limit passed to underlying service to specify the max …\nThe limit passed to underlying service to specify the max …\nThe limit passed to underlying service to specify the max …\nThe limit passed to underlying service to specify the max …\nRefer to <code>options::ReadOptions::override_cache_control</code> for …\nRefer to <code>options::StatOptions::override_cache_control</code> for …\nRefer to <code>options::StatOptions::override_cache_control</code> for …\nRefer to <code>options::ReadOptions::override_cache_control</code> for …\nRefer to <code>options::StatOptions::override_content_disposition</code>…\nRefer to <code>options::ReadOptions::override_content_disposition</code>…\nRefer to <code>options::StatOptions::override_content_disposition</code>…\nRefer to <code>options::ReadOptions::override_content_disposition</code>…\nRefer to <code>options::ReadOptions::override_content_type</code> for …\nRefer to <code>options::StatOptions::override_content_type</code> for …\nRefer to <code>options::StatOptions::override_content_type</code> for …\nRefer to <code>options::ReadOptions::override_content_type</code> for …\nSet <code>range</code> for this <code>read</code> request.\nSet <code>range</code> for this <code>read</code> request.\nThe recursive is used to control whether the list …\nThe recursive is used to control whether the list …\nThe recursive is used to control whether the list …\nThe recursive is used to control whether the list …\nThe start_after passes to underlying service to specify …\nThe start_after passes to underlying service to specify …\nThe start_after passes to underlying service to specify …\nThe start_after passes to underlying service to specify …\nSets user metadata for this write request.\nSets user metadata for this write request.\nSets user metadata for this write request.\nSets user metadata for this write request.\nSet the version for this operation.\nSet <code>version</code> for this <code>read</code> request.\nChange the version of this delete operation.\nSet <code>version</code> for this <code>reader</code>.\nSet the version for this operation.\nSet <code>version</code> for this <code>read</code> request.\nSet <code>version</code> for this <code>reader</code>.\nChange the version of this delete operation.\nControls whether the <code>list</code> operation should return file …\nControls whether the <code>list</code> operation should return file …\nControls whether the <code>list</code> operation should return file …\nControls whether the <code>list</code> operation should return file …\nOptions for delete operations.\nOptions for list operations.\nOptions for read operations.\nOptions for reader operations.\nOptions for stat operations.\nOptions for write operations.\nSets append mode for this operation.\nSets Cache-Control header for this write operation.\nSet <code>chunk</code> for the operation.\nSet <code>chunk</code> for the operation.\nSets chunk size for buffered writes.\nSet <code>concurrent</code> for the operation.\nSet <code>concurrent</code> for the operation.\nSets concurrent write operations for this writer.\nSets Content-Disposition header for this write request.\nSets Content-Encoding header for this write request.\nSets <code>Content-Type</code> header for this write operation.\nThe deleted is used to control whether the deleted objects …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nControls the optimization strategy for range reads in […\nControls the optimization strategy for range reads in […\nSet <code>if_match</code> for this operation.\nSet <code>if_match</code> for this operation.\nSet <code>if_match</code> for this operation.\nSets If-Match header for this write request.\nSet <code>if_modified_since</code> for this operation.\nSet <code>if_modified_since</code> for this operation.\nSet <code>if_modified_since</code> for this operation.\nSet <code>if_none_match</code> for this operation.\nSet <code>if_none_match</code> for this operation.\nSet <code>if_none_match</code> for this operation.\nSets If-None-Match header for this write request.\nSets the condition that write operation will succeed only …\nSet <code>if_unmodified_since</code> for this operation.\nSet <code>if_unmodified_since</code> for this operation.\nSet <code>if_unmodified_since</code> for this operation.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nThe limit passed to underlying service to specify the max …\nSpecify the <code>cache-control</code> header that should be sent back …\nSpecify the <code>cache-control</code> header that should be sent back …\nSpecify the <code>content-disposition</code> header that should be sent …\nSpecify the <code>content-disposition</code> header that should be sent …\nSpecify the content-type header that should be sent back …\nSpecify the content-type header that should be sent back …\nSet <code>range</code> for this operation.\nThe recursive is used to control whether the list …\nThe start_after passes to underlying service to specify …\nSets user metadata for this write request.\nThe version of the file to delete.\nSet <code>version</code> for this operation.\nSet <code>version</code> for this operation.\nSet <code>version</code> for this operation.\nThe version is used to control whether the object versions …\nUnderlying trait of all backends for implementers.\n<code>AccessDyn</code> is the dyn version of <code>Access</code> make it possible to …\nAccessor is the type erased accessor with <code>Arc&lt;dyn Accessor&gt;</code>…\nInfo for the accessor. Users can use this struct to …\nAtomicContentLength is a wrapper of AtomicU64 that used to …\nBoxedFuture is the type alias of <code>futures::future::BoxFuture</code>…\nBoxedStaticFuture is the type alias of …\nBytesContentRange is the content range of bytes.\nBytesRange(offset, size) carries a range of content.\nConcurrentTasks is used to execute tasks concurrently.\nConfigDeserializer is used to deserialize given configs …\nOperation to copy a file.\nOperation to create a directory.\nOperation to delete files.\nPresign a delete operation.\nDeleter is the associated deleter returned in <code>delete</code> …\nFormDataPart is a builder for multipart/form-data part.\nThe fourth type for the <code>FourWays</code>.\nFourWays is used to implement traits that based on four …\nThe streaming body that OpenDAL’s HttpClient returned.\nA HTTP client instance for OpenDAL’s services.\nHttpFetch is the trait to fetch a request in async way. …\nHttpFetcher is a type erased <code>HttpFetch</code>.\nOperation to retrieve information about the specified …\nLayer is used to intercept the operations on the …\nLayeredAccess is layered accessor that forward all not …\nThe layered accessor that returned by this layer.\nOperation to get the next file from the list.\nLister is the associated lister returned in <code>list</code> operation.\nMaybeSend is a marker to determine whether a type is <code>Send</code> …\nMixedPart is a builder for multipart/mixed part.\nMultipart is a builder for multipart/form-data.\nThe first type for the <code>TwoWays</code>.\nThe first type for the <code>ThreeWays</code>.\nThe first type for the <code>FourWays</code>.\nArgs for <code>copy</code> operation.\nArgs for <code>create</code> operation.\nArgs for <code>delete</code> operation.\nArgs for <code>delete</code> operation.\nArgs for <code>list</code> operation.\nArgs for <code>presign</code> operation.\nArgs for <code>read</code> operation.\nArgs for reader operation.\nArgs for <code>rename</code> operation.\nArgs for <code>stat</code> operation.\nArgs for <code>write</code> operation.\nArgs for <code>writer</code> operation.\nOperation is the name of the operation that is being …\nPart is a trait for multipart part.\nPathCacher is a cache for path query.\nThe trait required for path cacher.\nOperation to generate a presigned URL.\nPresign operation used for presign.\nPresignedRequest is a presigned request return by <code>presign</code>.\nQueryPairsWriter is used to write query pairs to a url.\nOperation to read a file.\nPresign a read operation.\nReader is the associated reader returned in <code>read</code> operation.\nRelatedPart is a builder for multipart/related part.\nOperation to rename a file.\nReply for <code>copy</code> operation.\nReply for <code>create_dir</code> operation\nReply for <code>delete</code> operation\nReply for <code>list</code> operation.\nReply for <code>presign</code> operation.\nReply for <code>read</code> operation.\nReply for <code>rename</code> operation.\nReply for <code>stat</code> operation.\nReply for <code>write</code> operation.\nOperation to stat a file or a directory.\nPresign a stat(head) operation.\nTYPE is the type of multipart.\nThe third type for the <code>ThreeWays</code>.\nThe third type for the <code>FourWays</code>.\nThreeWays is used to implement traits that based on three …\nThe second type for the <code>TwoWays</code>.\nThe second type for the <code>ThreeWays</code>.\nThe second type for the <code>FourWays</code>.\nTwoWays is used to implement traits that based on two ways.\nVERSION is the compiled version of OpenDAL.\nOperation to write to a file.\nPresign a write operation.\nWriter is the associated writer returned in <code>write</code> …\nProviding adapters and its implementations.\nAdvance the range by <code>n</code> bytes.\nGet the append from op.\nConsume the input and generate a request with multipart …\nBuild a new http client in async context.\nbuild_abs_path will build an absolute path with root.\nBuild header value from given string.\nbuild_rel_path will build a relative path towards root.\nbuild_rooted_abs_path will build an absolute path with …\nBuild a temporary path of a file path.\nGet the cache control from option\nGet chunk from option\nGet the chunk from op.\nClear all tasks and results.\nGet the concurrent of list operation.\nGet concurrent from option\nGet the concurrent.\nSet the content for this part.\nSet the content for this part.\nSet the content for this part.\nGet the content disposition from option\nGet the content encoding from option\nGet the content type from option\nInvoke the <code>copy</code> operation on the specified <code>from</code> path and <code>to</code>…\nInvoke the <code>copy</code> operation on the specified <code>from</code> path and <code>to</code>…\nDyn version of <code>Accessor::copy</code>\nCreate a dir by parent_id and name.\nInvoke the <code>create</code> operation on the specified path\nInvoke the <code>create</code> operation on the specified path\nDyn version of <code>Accessor::create_dir</code>\nCreate a task with given input.\nInvoke the <code>delete</code> operation on the specified path.\nInvoke the <code>delete</code> operation on the specified path.\nDyn version of <code>Accessor::delete</code>\nGet the deleted of this list operation\nEnsure input dir exists.\nExecute the task with given input.\nGet executor from the context.\nGet expire from op.\nFetch a request in async way.\nFetch a request and return a streamable <code>HttpBody</code>.\nFinish the url and return it.\nformat will generates the bytes.\nformat authorization header by basic auth.\nformat authorization header by bearer token.\nformat content md5 header by given input.\nformat datetime into http date, this format is required by:\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nBuild a mixed part from a request.\nBuild a mixed part from a request.\nGet service’s full capabilities.\nGet gap from option\nGet the id for the given path.\nGet basename from path.\nGet parent from path.\nCheck if there are remaining space to push new tasks.\nChunk if there are remaining results to fetch.\nInsert a header into part.\nInsert a header into part.\nInsert a header into part.\nReturn request’s header.\nGet http client from the context.\nGet If-Match from option\nGet If-Match from option\nGet If-Match from option\nGet If-Modified-Since from option\nGet If-Modified-Since from option\nGet If-None-Match from option\nGet If-None-Match from option\nGet If-None-Match from option\nGet If-Not-Exist from option\nGet If-Unmodified-Since from option\nGet If-Unmodified-Since from option\nInvoke the <code>info</code> operation to get metadata of accessor.\nDyn version of <code>Accessor::info</code>\nInsert a new cache entry.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConsume RpStat to get the inner metadata.\nConsume OpPresign into (Duration, PresignOperation)\nInto parts.\nConsume reply to build a presigned request.\nConsume a mixed part to build a response.\nConsume a mixed part to build a response.\nConvert self into static str.\nCheck if this range is full of this content.\nIntercept the operations on the underlying storage.\nGet the length that specified by this BytesContentRange, …\nGet the limit of list operation.\nInvoke the <code>list</code> operation on the specified path.\nInvoke the <code>list</code> operation on the specified path.\nDyn version of <code>Accessor::list</code>\nLoad content length from AtomicU64.\nOperate on inner metadata.\nSet the method for request in this part.\nReturn request’s method.\nName of backend, could be empty if underlying backend doesn…\nGet backend’s native capabilities.\nCreate a new path cacher.\nCreate a new <code>HttpBody</code> with given stream and optional size.\nCreate a new QueryPairsWriter with the given base.\nCreate a new part builder\nCreate a new mixed part with given uri.\nCreate a new related\nCreate a new config deserializer.\nCreate a new concurrent tasks with given executor, …\nTODO: maybe we can move the signed url logic to azblob …\nCreate a new reply for <code>presign</code>.\nCreate a new PresignedRequest\nCreate a new reply for <code>read</code>.\nCreate a new reply for <code>stat</code>.\nCreate a new reply for <code>write</code>.\nCreate a new reply for <code>copy</code>.\nCreate a new reply for <code>rename</code>.\nCreate a new <code>OpCreateDir</code>.\nCreate a new <code>OpDelete</code>.\nCreate a new <code>OpDelete</code>.\nCreate a new <code>OpList</code>.\nCreate a new <code>OpPresign</code>.\nCreate a default <code>OpRead</code> which will read whole content of …\nCreate a new <code>OpReader</code>.\nCreate a new <code>OpStat</code>.\nCreate a new <code>OpWrite</code>.\nCreate a new <code>OpWriter</code>.\nCreate a new <code>OpCopy</code>.\nCreate a new <code>OpMove</code>.\nCreate a new http client in async context.\nCreate a new <code>BytesRange</code>\nCreate a new multipart with random boundary.\nCreate a new AtomicContentLength.\nParse http uri invalid error in to opendal::Error.\nParse json deserialize error into opendal::Error.\nParse json serialize error into opendal::Error.\nCreate a new error happened during building request.\nCreate a new error happened during signing request.\nCreate a new error happened during signing request.\nParse std io error into opendal::Error.\nParse tokio error into opendal::Error.\nParse xml deserialize error into opendal::Error.\nParse xml serialize error into opendal::Error.\nFetch the successful result from the result queue.\nMake sure all operation are constructed by normalized path:\nMake sure root is normalized to style like <code>/abc/def/</code>.\nGet offset of BytesRange.\n<code>oio</code> provides OpenDAL’s raw traits and types that opendal …\nGet operation from op.\nReturns the cache-control header that should be sent back …\nReturns the cache-control header that should be sent back …\nReturns the content-disposition header that should be sent …\nReturns the content-disposition header that should be sent …\nReturns the content-type header that should be sent back …\nReturns the content-type header that should be sent back …\nparse will parse the bytes into a part.\nTODO\nParse a response with multipart body into Multipart.\nParse Content-Disposition for header map\nParse content encoding from header map.\nParse content length from header map.\nParse content md5 from header map.\nParse content range from header map.\nParse content type from header map.\nparse datetime from given timestamp\nparse datetime from given timestamp_millis\nParse datetime from rfc2822.\nParse datetime from rfc3339.\nParse etag from header map.\nParse header value to string according to name.\nparse_into_metadata will parse standards http headers into …\nParse last modified from header map.\nParse redirect location from header map\nParse multipart boundary from header map.\nParse prefixed headers and return a map with the prefix of …\nInsert a part into multipart.\nInsert a part header into part.\npercent_decode_path will do percent decoding for http …\npercent_encode_path will do percent encoding for http …\nInvoke the <code>presign</code> operation on the specified path.\nInvoke the <code>presign</code> operation on the specified path.\nDyn version of <code>Accessor::presign</code>\nPush a new pair of key and value to the url.\nQuery the id by parent_id and name.\nGot the range of the reader returned by this read …\nGet range from option\nGet the range inclusive of this BytesContentRange, return …\nGet the range inclusive of this BytesContentRange, return …\nInvoke the <code>read</code> operation on the specified path, returns a …\nInvoke the <code>read</code> operation on the specified path, returns a …\nDyn version of <code>Accessor::read</code>\nGet the current recursive.\nRemove a cache entry.\nInvoke the <code>rename</code> operation on the specified <code>from</code> path and …\nInvoke the <code>rename</code> operation on the specified <code>from</code> path and …\nDyn version of <code>Accessor::rename</code>\nFetch the id for the root of the service.\nRoot of backend, will be in format like <code>/path/to/dir/</code>\n<code>Scheme</code> of backend.\nSend a request and consume response.\nSet name of this backend.\nSet native capabilities for service.\nSet root for backend.\nSet <code>Scheme</code> for backend.\nGot the size of the reader returned by this read operation.\nGet size of BytesRange.\nGet the size of this BytesContentRange, return <code>None</code> if …\nGet the start_after of list operation.\nInvoke the <code>stat</code> operation on the specified path.\nInvoke the <code>stat</code> operation on the specified path.\nDyn version of <code>Accessor::stat</code>\nStore content length to AtomicU64.\nUtilities for opendal testing.\nRead all data from the stream.\nConvert bytes range into Range header.\nConvert bytes content range into Content-Range header.\nConvert bytes range into rust range.\nUpdate executor for the context.\nUpdate service’s full capabilities.\nUpdate http client for the context.\nReturn request’s uri.\nGet the user defined metadata from the op\nValidate given path is match with given EntryMode.\nSet the version for request in this part.\nGet the version of this delete operation.\nGet the version of this list operation\nGet version from option\nGet version from option\nGet the version of this list operation\nConstruct <code>Self</code> with given <code>reqwest::Client</code>\nSet the append mode of op.\nSet the boundary with given string.\nSet the content type of option\nSet the chunk of the option\nSet the chunk of op.\nChange the concurrent of this list operation.\nSet the concurrent of the option\nSet the maximum concurrent write task amount.\nSet the content disposition of option\nSet the content encoding of option\nSet the content type of option\nChange the deleted of this list operation\nAdd response context to error.\nSet the gap of the option\nSet the If-Match of the option\nSet the If-Match of the option\nSet the If-Match of the option\nSet the If-Modified-Since of the option\nSet the If-Modified-Since of the option\nSet the If-None-Match of the option\nSet the If-None-Match of the option\nSet the If-None-Match of the option\nSet the If-Not-Exist of the option\nSet the If-Unmodified-Since of the option\nSet the If-Unmodified-Since of the option\nChange the limit of this list operation.\nEnable the lock for the path cacher.\nSets the cache-control header that should be sent back by …\nSets the cache-control header that should be sent back by …\nSets the content-disposition header that should be sent …\nSets the content-disposition header that should be sent …\nSets the content-type header that should be sent back by …\nSets the content-type header that should be sent back by …\nSet the range of the reader returned by this read …\nSet the range of the option\nUpdate BytesContentRange with range.\nThe recursive is used to control whether the list …\nSet the size of the reader returned by this read operation.\nUpdate BytesContentRange with size.\nChange the start_after of this list operation.\nSet the user defined metadata of the op\nChange the version of this delete operation.\nChange the version of this list operation\nSet the version of the option\nSet the version of the option\nChange the version of this list operation\nInvoke the <code>write</code> operation on the specified path, returns a\nInvoke the <code>write</code> operation on the specified path, returns a\nDyn version of <code>Accessor::write</code>\nProviding Key Value Adapter for OpenDAL.\nProviding Typed Key Value Adapter for OpenDAL.\nKvAdapter is the adapter to underlying kv services.\nBackend of kv service. If the storage service is one …\nInfo for this key value accessor.\nScan is the async iterator returned by <code>Adapter::scan</code>.\nA type-erased wrapper of Scan\nTODO: use default associate type <code>= ()</code> after stabilized\nAppend a key into service\nAppend a key into service\nGet the capabilities.\nDelete a key from service.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet a key from service.\nReturn the info of this key value accessor.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nGet the name.\nCreate a new KeyValueAccessorInfo.\nCreate a new kv backend.\nFetch the next key in the current key prefix\nScan a key prefix to get all keys that start with this key.\nScan a key prefix to get all keys that start with this key.\nGet the scheme.\nSet a key into service.\nConfigure root within this backend.\nAdapter is the typed adapter to underlying kv services.\nThe typed kv backend which implements Accessor for typed …\nCapability is used to describe what operations are …\nInfo for this key value accessor.\nValue is the typed value stored in adapter.\nGet the capabilities.\nDelete a value from adapter.\nIf typed_kv operator supports delete natively.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet a value from adapter.\nIf typed_kv operator supports get natively.\nReturn the info of this key value accessor.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nMetadata of this value.\nGet the name.\nCreate a new KeyValueAccessorInfo.\nCreate a new kv backend.\nCreate a new dir of value.\nScan a key prefix to get all keys that start with this key.\nScan a key prefix to get all keys that start with this key.\nIf typed_kv operator supports scan natively.\nGet the scheme.\nSet a value into adapter.\nIf typed_kv operator supports set natively.\nIf typed_kv operator supports shared access.\nSize returns the in-memory size of Value.\nThe corresponding content of this value.\nConfigure root within this backend.\nAppendWrite is used to implement <code>oio::Write</code> based on append\nAppendWriter will implements <code>oio::Write</code> based on append …\nBatchDelete is used to implement <code>oio::Delete</code> based on …\nBatchDeleteResult is the result of batch delete operation.\nBatchDeleter is used to implement <code>oio::Delete</code> based on …\nBlockWrite is used to implement <code>oio::Write</code> based on block …\nBlockWriter will implement <code>oio::Write</code> based on block …\nThe Delete trait defines interfaces for performing …\nThe dyn version of <code>Delete</code>\nDeleter is a type erased <code>Delete</code>\nEntry is returned by <code>Page</code> or <code>BlockingPage</code> during list …\nFlatLister will walk dir in bottom up way:\nFlexBuf is a buffer that support frozen bytes and reuse …\nToHierarchyLister will convert a flat list to hierarchy by …\nPage trait is used by <code>raw::Accessor</code> to implement <code>list</code> …\nThe boxed version of <code>List</code>\nThe result of <code>MultipartWrite::write_part</code>.\nMultipartWrite is used to implement <code>oio::Write</code> based on …\nMultipartWriter will implement <code>oio::Write</code> based on …\nOneShotDelete is used to implement <code>oio::Delete</code> based on …\nOneShotDelete is used to implement <code>oio::Delete</code> based on …\nOneShotWrite is used to implement <code>oio::Write</code> based on one …\nOneShotWrite is used to implement <code>oio::Write</code> based on one …\nPageContext is the context passing between <code>PageList</code>.\nPageList is used to implement <code>oio::List</code> based on API …\nPageLister implements <code>oio::List</code> based on <code>PageList</code>.\nPooledBuf is a buffer pool that designed for reusing …\nPositionWrite is used to implement <code>oio::Write</code> based on …\nPositionWriter will implement <code>oio::Write</code> based on position …\nPrefixLister is used to filter entries by prefix.\nQueueBuf is a queue of <code>Buffer</code>.\nRead is the internal trait used by OpenDAL to read data …\nReadDyn is the dyn version of <code>Read</code> make it possible to use …\nReader is a type erased <code>Read</code>.\nWrite is the trait that OpenDAL returns to callers.\nWriter is a type erased <code>Write</code>\nAbort the pending writer.\nabort is used to abort the underlying abort.\nabort_block will cancel the block upload and purge all …\nabort_part will cancel the multipart upload and purge all …\nPanics\nAdvance the buffer queue by <code>cnt</code> bytes.\nAppend the data to the end of this object.\nThe checksum of the part.\nCleanup the buffer, reset to the initial state.\nClear the buffer queue.\nClose the writer and make sure all data has been flushed.\nclose is used to close the underlying file.\nBuild a new <code>Buffer</code> from the queue.\ncomplete_block will complete the block upload to build the …\ncomplete_part will complete the multipart upload to build …\nRequests deletion of a resource at the specified path with …\ndelete_batch delete multiple paths at once.\nThe dyn version of <code>Delete::delete</code>\ndelete_once delete one path at once.\ndelete_once delete one path at once.\ndone is used to indicate whether the list operation is …\nentries are used to store entries fetched from underlying …\nThe etag of the part.\nCollection of failed deletions, containing tuples of …\nFlushes the deletion queue to ensure queued deletions are …\nThe dyn version of <code>Delete::flush</code>\nFreeze the buffer no matter it’s full or not.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nGet the frozen buffer.\nGet a <code>BytesMut</code> from the pool.\ninitiate_part will call start a multipart upload and …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nIs the buffer queue empty.\nTotal bytes size inside the buffer queue.\nGet entry’s mode.\nCreate a new batch deleter.\nCreate a new one shot deleter.\nCreate a new MultipartWriter.\nCreate a new AppendWriter.\nCreate a new one shot writer.\nCreate a new BlockWriter.\nCreate a new PositionWriter.\nCreate a new PageLister.\nCreate a new flat lister\nCreate a new hierarchy lister\nCreate a new flat lister\nInitializes a new <code>FlexBuf</code> with the given capacity.\nCreate a new buffer pool with a given size.\nCreate a new entry by its corresponding underlying storage.\nCreate a new buffer queue.\nFetch a new page of <code>Entry</code>\nnext_page is used to fetch next page of entries from …\nGet the current offset of the append object.\nThe number of the part, starting from 0.\nGet the path of entry.\nPush new <code>Buffer</code> into the queue.\nPut slice into flex buf.\nPut a <code>BytesMut</code> back to the pool.\nRead at the given offset with the given size.\nRead all data from the reader.\nRead all data from the reader.\nThe dyn version of <code>Read::read_all</code>\nThe dyn version of <code>Read::read</code>.\nSet mode for entry.\nSet path for entry.\nCollection of successful deletions, containing tuples of …\nTake the entire buffer queue and leave <code>self</code> in empty …\ntoken is used by underlying storage services to fetch next …\nCreate a new entry with given value.\nSet the initial capacity of the buffer.\nWrite given bytes into writer.\nwrite_all_at is used to write the data to underlying …\nwrite_block will write a block of the data.\nwrite_once is used to write the data to underlying storage …\nwrite_once write all data at once.\nwrite_once is used to write the data to underlying storage …\nwrite_part will write a part of the data and returns the …\nRead represents a read action with given input buf size.\nReadAction represents a read action.\nReadChecker is used to check the correctness of the read …\nTEST_RUNTIME is the runtime used for running tests.\nWrite represents a write action with given input buf size.\nWriteAction represents a read action.\nWriteAction is used to check the correctness of the write …\nCheck will check the correctness of the read process via …\nCheck the correctness of the write process.\nGet the check’s chunks.\nReturn the raw data of this read checker.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nInit a service with given scheme.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new read checker by given size and range.\nCreate a new WriteChecker with given size.\nCapabilities\nConfig for Aliyun Drive services support.\nAlluxio services support.\nConfig for alluxio services support.\nCapabilities\nAzure Storage Blob services support.\nAzure Data Lake Storage Gen2 Support. As known as <code>abfs</code>, …\nAzure Data Lake Storage Gen2 Support.\nAzure File services support.\nAzure File services support.\nb2 services support.\nConfig for backblaze b2 services support.\ncacache service support.\ncacache service support.\nCapabilities\nCloudflare KV Service Support.\n<code>compio</code>-based file system support.\ncompio-based file system support.\nTencent-Cloud COS services support.\nTencent-Cloud COS services support.\nCapabilities\nConfig for Cloudflare D1 backend support.\ndashmap backend support.\nConfig for Dashmap services support.\nDbfs’s REST API support. This service will visit the …\nDbfs’s REST API support.\nDropbox backend support.\nConfig for Dropbox backend support.\nEtcd services support.\nConfig for Etcd services support.\nCapabilities\nfoundationdb service support. Config for FoundationDB.\nPOSIX file system support.\nconfig for file system\nFTP and FTPS services support.\nConfig for Ftp services support.\nGoogle Cloud Storage services support.\nGoogle Cloud Storage services support.\nGoogleDrive backend support.\nGoogleDrive configuration.\nGitHub Action Cache Services support.\nConfig for GitHub Action Cache Services support.\ngithub contents services support.\nConfig for GitHub services support.\nCapabilities\nConfig for Grid file system support.\nA distributed file system that provides high-throughput …\nHadoop Distributed File System (HDFS™) support.\nA distributed file system that provides high-throughput …\nConfig for HdfsNative services support.\nHTTP Read-only service support like Nginx and Caddy.\nConfig for Http service support.\nHuggingface’s API support. This service will visit the …\nConfiguration for Huggingface service support.\nIPFS file system support based on IPFS HTTP Gateway.\nConfig for IPFS file system support.\nIPFS file system support based on IPFS MFS API.\nConfig for IPFS MFS support.\nKoofr services support.\nConfig for Koofr services support.\nLakefs’s API support. This service will visit the Lakefs …\nConfiguration for Lakefs service support.\nMemcached service support.\nConfig for MemCached services support\nIn memory service support. (BTreeMap Based)\nConfig for memory.\nmini-moka backend support.\nConfig for mini-moka support.\nmoka backend support.\nType alias of <code>moka::future::CacheBuilder</code>\nConfig for Moka services support.\nValue stored in moka cache containing both metadata and …\nCapabilities\nConfig for Mongodb service support.\nFile system support via <code>monoio</code>.\nConfig for monoiofs services support.\nCapabilities\nConfig for Mysql services support.\nHuawei-Cloud Object Storage Service (OBS) support\nConfig for Huawei-Cloud Object Storage Service (OBS) …\nMicrosoft OneDrive backend support.\nConfig for OneDrive backend support.\nAliyun Object Storage Service (OSS) support\nConfig for Aliyun Object Storage Service (OSS) support.\npCloud services support.\nConfig for Pcloud services support.\npersy service support.\nConfig for persy service support.\nPostgreSQL services support.\nConfig for PostgreSQL services support.\nRedb service support.\nConfig for redb service support.\nRedis services support.\nConfig for Redis services support.\nRocksDB service support.\nConfig for Rocksdb Service.\nAws S3 and compatible services (including minio, …\nConfig for Aws S3 and compatible services (including …\nseafile services support.\nConfig for seafile services support.\nSFTP services support. (only works on unix)\nConfig for Sftp Service support.\nSled services support.\nConfig for Sled services support.\nCapabilities\nConfig for Sqlite support.\nCapabilities\nConfig for Surrealdb services support.\nOpenStack Swift’s REST API support. For more information …\nConfig for OpenStack Swift support.\nTiKV backend builder\nConfig for Tikv services support.\nupyun services support.\nConfig for upyun services support.\nVercel Cache backend support.\nConfig for Vercel Cache support.\nVercelBlob services support.\nConfig for VercelBlob services support.\nWebDAV backend support.\nConfig for WebDAV backend support.\nWebHDFS’s REST API support. There two implementations of …\nConfig for WebHDFS support.\nYandexDisk services support.\nConfig for YandexDisk services support.\nSet access_key_id of this backend.\nSet access_key_id of this backend.\nSet access_key_id of this backend.\nAccess key id for obs.\nAccess key id for oss.\naccess_key_id of this backend.\nSet access_key_secret of this backend.\nAccess key secret for oss.\nSet access_token of this backend.\nAccess token is used for temporary access to the Dropbox …\nAccess token is used for temporary access to the …\nSet the access token for a time limited access to …\nset the bearer access token for Vercel\nyandex disk oauth access_token. The valid token will looks …\nThe access_token of this backend.\naccess token for dropbox.\nAccess token for gdrive.\nMicrosoft Graph API (also OneDrive API) access token\nThe access token for Vercel.\nyandex disk oauth access_token.\nSet the account ID used to authenticate with CloudFlare.\nSet the account identifier for the cloudflare d1 service.\nThe account ID used to authenticate with CloudFlare. Used …\nSet the account id of cloudflare api.\nSet account_key of this backend.\nSet account_key of this backend.\nSet account_key of this backend.\nThe account key of Azblob service backend.\nAccount key of this backend.\nThe account key for azfile.\nSet account_name of this backend.\nSet account_name of this backend.\nSet account_name of this backend.\nThe account name of Azblob service backend.\nAccount name of this backend.\nThe account name for azfile.\nAllow anonymous requests.\nAllow anonymous will allow opendal to send request without …\nAllow anonymous will allow opendal to send request without …\nAllow opendal to send requests without signing when …\nAllow anonymous for oss.\nAllow anonymous will allow opendal to send request without …\napplication_key of this backend.\napplicationKey of this backend.\napplication_key_id of this backend.\nkeyID of this backend.\nSet temp dir for atomic write.\nSet temp dir for atomic write.\nSet temp dir for atomic write.\ntmp dir for atomic write\natomic_write_dir of this backend\natomic_write_dir of this backend\nSet authority_host of this backend.\nauthority_host The authority host of the service principal.\nSet maximum batch operations of this backend.\nSet maximum batch operations of this backend.\nSet maximum batch operations of this backend.\nThe maximum batch operations of Azblob service backend.\nThe size of max batch operations.\nSet maximum batch operations of this backend.\nSet branch of this backend or a commit ID. Default is main.\nName of the branch or a commit ID. Default is main.\nSet bucket name of this backend. You can find it in …\nSet bucket of this backend. The param is required.\nset the container’s name\nSet the bucket name of the MongoDB GridFs service to …\nSet bucket of this backend. The param is required.\nSet bucket name of this backend.\nSet bucket name of this backend.\nbucket of this backend.\nbucket of this backend.\nBucket of this backend.\nbucket name\nThe bucket name of the MongoDB GridFs service to …\nBucket for obs.\nBucket for oss.\nbucket name of this backend.\nbucket address of this backend.\nSet bucket id of this backend. You can find it in …\nbucket id of this backend.\nBuilds the backend and returns the result of …\nBuilds the backend and returns the result of B2Backend.\nBuild a DbfsBackend.\nBuilds the backend and returns the result of GithubBackend.\nBuild a HuggingfaceBackend.\nBuilds the backend and returns the result of KoofrBackend.\nBuild a LakefsBackend.\nBuilds the backend and returns the result of PcloudBackend.\nBuilds the backend and returns the result of …\nBuild a SwiftBackend.\nBuilds the backend and returns the result of UpyunBackend.\nBuilds the backend and returns the result of …\nbuild the backend\nBuilds the backend and returns the result of …\nSet the certificate authority file path.\nSet the certificate authority file path.\ncertificate authority file path\ncertificate authority file path\nSet the certificate file path.\nSet the certificate file path.\ncert path\ncert path\nSet checksum algorithm of this backend. This is necessary …\nChecksum Algorithm to use when sending checksums in HTTP …\nSet the chunk size of the MongoDB GridFs service used to …\nThe chunk size of the MongoDB GridFs service used to break …\nSet client_id of this backend.\nSet client_id of this backend.\nSet the client id for Dropbox.\nSet the client id for GoogleDrive.\nSet the client_id for a Microsoft Graph API application …\nThe client_id of this backend.\nclient_id The client id of the service principal.\nclient_id for dropbox.\nClient id for gdrive.\nMicrosoft Graph API Application (client) ID that is in the …\nSet client_secret of this backend.\nSet client_secret of this backend.\nSet the client secret for Dropbox.\nSet the client secret for GoogleDrive.\nSet the client_secret for a Microsoft Graph API application\nThe client_secret of this backend.\nclient_secret The client secret of the service principal.\nclient_secret for dropbox.\nClient secret for gdrive.\nMicrosoft Graph API Application client secret that is in …\nset the network address of redis cluster service. This …\nnetwork address of the Redis cluster service. Can be “…\nSet the collection name of the MongoDB service to …\ncollection of this backend\nSet the config path for Foundationdb. If not set, will …\nconfig_path for the backend.\nSet the connection_string of the MongoDB service.\nSet the connection_string of the MongoDB service.\nSet the connection_string of the mysql service.\nSet the connection url string of the postgresql service.\nSet the connection_string of the sqlite service.\nSet the connection_string of the surrealdb service.\nThe connection string of the MongoDB service.\nconnection string of this backend\nThis connection string is used to connect to the mysql …\nThe URL should be with a scheme of either <code>postgres://</code> or …\nSet the connection_string of the sqlite service.\nThe connection string for surrealdb.\nSet container name of this backend.\nSet container of this backend.\nThe container name of Azblob service backend.\nThe container for Swift.\nStored content in moka cache.\nset the base64 hashed credentials string used for OAuth2 …\nCredentials string for GCS service OAuth2 authentication.\nset the local path to credentials file which is used for …\nLocal path to credentials file for GCS service OAuth2 …\nAdding a customized credential load for service.\nSpecify the customized token loader used by this service.\nSet the database name of the MongoDB GridFs service to …\nSet the database name of the MongoDB service to read/write.\nSet the database for Redb.\nSet the database of the surrealdb service for read/write.\nThe database name of the MongoDB GridFs service to …\ndatabase of this backend\nThe database for surrealdb.\nSet the database identifier for the cloudflare d1 service.\nSet the database id of cloudflare api.\nSet the path to the cacache data directory. Will create if …\nSet the path to the redb data directory. Will create if …\nSet the path to the rocksdb data directory. Will create if …\nSet the path to the sled data directory. Will create if …\nThat path to the cacache data directory.\npath to the redb data directory.\nThe path to the rocksdb data directory.\nThat path to the sled data directory.\nSet the path to the persy data directory. Will create if …\nThat path to the persy data file. The directory in the …\nset the db used in redis\nthe number of DBs redis can take is unlimited\nSet the default storage class for GCS.\nSet default storage_class for this backend.\nThe default storage class used by gcs.\ndefault storage_class for this backend.\nSet the default ttl for memcached services.\nSet the default ttl for redis services.\nThe default ttl for put operations.\nThe default ttl for put operations.\nSet the delegation token of this backend, used for …\nDelegation token for webhdfs.\nSet maximum delete operations of this backend.\nSet maximum delete operations of this backend.\nThe size of max delete operations.\nSet the maximum delete size of this backend.\nDetect region of S3 bucket.\nDisable config load so that opendal will not load config …\nDisable loading configuration from the environment.\nDisable config load so that opendal will not load config …\nDisable config load so that opendal will not load config …\nDisable loading configuration from the environment.\nDisable config load so that opendal will not load config …\nWebDAV Service doesn’t support copy.\nDisable load credential from ec2 metadata.\nDisable load credential from ec2 metadata.\nDisable batch listing\nDisable batch listing\nDisable list objects v2 so that opendal will not use the …\nOpenDAL uses List Objects V2 by default to list objects. …\nDisable stat with override so that opendal will not send …\nDisable stat with override so that opendal will not send …\nDisable attempting to load credentials from the GCE …\nDisable attempting to load credentials from the GCE …\nDisable write with if match so that opendal will not send …\nDisable write with if match so that opendal will not send …\nSet drive_type of this backend.\nThe drive_type of this backend.\nemail.\nKoofr email.\nEnable append capacity of this backend.\nEnable append capacity of this backend.\nenable the append capacity\nenable the append capacity\nset enable_copy for sftp backend. It requires the server …\nenable_copy of this backend\nEnable request payer so that OpenDAL will send requests …\nIndicates whether the client agrees to pay for the …\nSet bucket versioning status for this backend\nSet bucket versioning status for this backend\nEnable versioning support for OneDrive\nSet bucket versioning status for this backend\nSet bucket versioning status for this backend\nis bucket versioning enabled for this bucket\nIs bucket versioning enabled for this bucket\nEnabling version support\nis bucket versioning enabled for this bucket\nis bucket versioning enabled for this bucket\nEnable virtual host style so that opendal will send API …\nEnable virtual host style so that opendal will send API …\nEnable write with append so that opendal will send write …\nEnable write with append so that opendal will send write …\nSet encryption_algorithm of this backend.\nThe encryption algorithm of Azblob service backend.\nSet encryption_key of this backend.\nThe encryption key of Azblob service backend.\nSet encryption_key_sha256 of this backend.\nThe encryption key sha256 of Azblob service backend.\nendpoint of this backend.\nSet endpoint of this backend\nSet endpoint of this backend.\nSet endpoint of this backend.\nSet endpoint of this backend.\nSet endpoint of this backend.\nset endpoint for ftp backend.\nset the endpoint GCS service uses\nSet the endpoint for ghac service.\nSet endpoint for http backend.\nSet endpoint if ipfs backend.\nSet endpoint for ipfs.\nendpoint.\nSet the endpoint of this backend.\nset the network address of memcached service.\nSet endpoint of this backend.\nSet endpoint of this backend.\nPcloud endpoint. https://api.pcloud.com for United States …\nset the network address of redis service.\nSet endpoint of this backend.\nendpoint of this backend.\nset endpoint for sftp backend. The format is same as …\nSet the remote address of this backend\nSet endpoint for http backend.\nSet the remote address of this backend default to …\nendpoint of this backend.\nThe endpoint of Azblob service backend.\nEndpoint of this backend.\nThe endpoint for azfile.\nEndpoint of this backend.\nThe endpoint for dbfs.\nendpoint of this backend\nendpoint URI of GCS service, default is …\nThe endpoint for ghac service.\nendpoint of this backend\nIPFS gateway endpoint.\nEndpoint for ipfs.\nKoofr endpoint.\nBase url.\nnetwork address of the memcached service.\nEndpoint for obs.\nEndpoint for oss.\npCloud  endpoint address.\nnetwork address of the Redis service. Can be “…\nendpoint of this backend.\nendpoint address of this backend.\nendpoint of this backend\nThe endpoint for Swift.\nendpoint of this backend\nEndpoint for webhdfs.\nset the network address of etcd service.\nSet the network address of the TiKV service.\nnetwork address of the Etcd services. If use https, must …\nnetwork address of the TiKV service.\nSet external_id for this backend.\nexternal_id for this backend.\nSet filesystem name of this backend.\nFilesystem name of this backend.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nfrom_connection_string will make a builder from connection …\nCreate a new <code>AzdlsBuilder</code> instance from an Azure Storage …\nCreate a new <code>AfileBuilder</code> instance from an Azure Storage …\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSpecify the http client that used by this service.\nSet the name of the persy index. Will create if not exists.\nThat name of the persy index.\nSet the insecure connection to TiKV.\nwhether using insecure connection to TiKV\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nSet kerberos_ticket_cache_path of this backend\nkerberos_ticket_cache_path of this backend\nset key path for sftp backend.\nkey of this backend\nSet the key field name of the d1 service to read/write.\nSet the key field name of the MongoDB service to …\nSet the key field name of the mysql service to read/write.\nSet the key field name of the postgresql service to …\nSet the key field name of the sqlite service to read/write.\nSet the key field name of the surrealdb service for …\nSet the key field of D1 Database.\nkey field of this backend\nThe key field name for mysql.\nthe key field of postgresql\nSet the key field name of the sqlite service to read/write.\nThe key field for surrealdb.\nSet the key file path.\nSet the key file path.\nkey path\nkey path\nset known_hosts strategy for sftp backend. available …\nknown_hosts_strategy of this backend\nSets the max capacity of the cache.\nSets the max capacity of the cache.\nSets the max capacity of the cache.\nSets the max capacity of the cache.\nStored metadata in moka cache.\nSets the name of the cache.\nName for this cache instance.\nSet name_node of this backend.\nSet name_node of this backend.\nname node of this backend\nname_node of this backend\nSet the namespace of the surrealdb service for read/write.\nThe namespace for surrealdb.\nSet the namespace ID.\nThe namespace ID. Used as URI path parameter.\nCreate a <code>MiniMokaBuilder</code> with default configuration.\nCreate a <code>MokaBuilder</code> with the given …\nSet oidc_provider_arn for this backend.\n<code>oidc_provider_arn</code> will be loaded from\nSet oidc_token_file for this backend.\n<code>oidc_token_file</code> will be loaded from\noperator of this backend.\nusername of this backend.\nSet Github repo owner.\nGitHub repo owner.\nset the password for etcd\nset password for ftp backend.\nset password for http backend\nKoofr application password.\nSet password of this backend. This is required.\nset the password.\nPcloud password.\nset the password for redis\npassword of this backend.\nSet the password of the surrealdb service for signin.\npassword of this backend.\nset the password for Webdav\nthe password for authentication\npassword of this backend\npassword of this backend\npassword of this backend. (Must be the application …\nPassword for Lakefs basic authentication.\nMemcached password, optional.\npCloud password.\nthe password for authentication\npassword of this backend.\nThe password for surrealdb.\npassword of this backend.\npassword of this backend\nSet the predefined acl for GCS.\nThe predefined acl for GCS.\nSet an endpoint for generating presigned urls.\nPresign endpoint for oss.\nSet refresh_token of this backend.\nRefresh token is used for long term access to the Dropbox …\nRefresh token is used for long term access to the …\nSet the refresh token for long term access to Microsoft …\nThe refresh_token of this backend.\nrefresh_token for dropbox.\nRefresh token for gdrive.\nMicrosoft Graph API (also OneDrive API) refresh token\nRegion represent the signing region of this endpoint. This …\nRegion represent the signing region of this endpoint. This …\nSet Github repo name.\nGitHub repo name.\nSet repo id of this backend. This is required.\nRepo id of this backend.\nSet repo name of this backend.\nrepo_name of this backend.\nSet repo type of this backend. Default is model.\nRepo type of this backend. Default is model.\nSet the repository of this backend.\nThe repository name\nSet revision of this backend. Default is main.\nRevision of this backend.\nSet role_arn for this backend.\nSet role_arn for this backend.\nIf <code>role_arn</code> is set, we will use already known config as …\nrole_arn for this backend.\nSet role_session_name for this backend.\nSet role_session_name for this backend.\nrole_session_name for this backend.\nrole_session_name for this backend.\nSet the root of this backend.\nSet root of this backend.\nSet root of this backend.\nSet root of this backend.\nSet root of this backend.\nSet root of this backend.\nSet the root within this backend.\nSet root for Compfs\nSet root of this backend.\nset the working directory, all operations will be …\nSet the root for dashmap.\nSet root of this backend.\nSet the root directory for dropbox.\nset the working directory, all operations will be …\nSet the root for Foundationdb.\nSet root for backend.\nset root path for ftp backend.\nset the working directory root of backend\nSet root path of GoogleDrive folder.\nset the working directory root of backend\nSet root of this backend.\nSet the working directory, all operations will be …\nSet root of this backend.\nSet root of this backend.\nSet root path of http backend.\nSet root of this backend.\nSet root of ipfs backend.\nSet root for ipfs.\nSet root of this backend.\nSet root of this backend.\nset the working directory, all operations will be …\nSet the root for BTreeMap.\nSet root path of this backend\nSet the root path of this backend\nSet the working directory, all operations will be …\nSet root of this backend.\nset the working directory, all operations will be …\nSet root of this backend.\nSet root path of OneDrive folder.\nSet root of this backend.\nSet root of this backend.\nSet the working directory, all operations will be …\nSet the root for Redb.\nset the working directory, all operations will be …\nset the working directory, all operations will be …\nSet root of this backend.\nSet root of this backend.\nset root path for sftp backend. It uses the default …\nSet the root for sled.\nset the working directory, all operations will be …\nset the working directory, all operations will be …\nSet root of this backend.\nSet root of this backend.\nSet root of this backend.\nSet root path of http backend.\nSet the working directory of this backend\nSet root of this backend.\nThe Root of this backend.\nroot of this backend.\nThe root of Azblob service backend.\nRoot of this backend.\nThe root path for azfile.\nroot of this backend.\nRoot within this backend.\nroot of this backend.\nRoot of this backend.\nSet the working directory of OpenDAL.\nroot path of this backend\nThe root for dbfs.\nroot path for dropbox.\nthe working directory of the etcd service. Can be “…\nroot of the backend.\nroot dir for backend\nroot of this backend\nroot URI, all operations happens under <code>root</code>\nThe root for gdrive\nThe root path for ghac.\nroot of this backend.\nThe working directory, all operations will be performed …\nwork dir of this backend\nwork dir of this backend\nroot of this backend\nRoot of this backend. Can be “/path/to/dir”.\nIPFS root.\nRoot for ipfs.\nroot of this backend.\nRoot of this backend. Can be “/path/to/dir”.\nthe working directory of the service. Can be “…\nroot of the backend.\nroot path of this backend\nroot path of this backend\nroot of this backend\nThe Root of this backend.\nThe root for mysql.\nRoot for obs.\nThe root path for the OneDrive service for the file access\nRoot for oss.\nroot of this backend.\nRoot of this backend.\nThe root for redb.\nthe working directory of the Redis service. Can be “…\nthe working directory of the service. Can be “…\nroot of this backend.\nroot of this backend.\nroot of this backend\nThe root for sled.\nset the working directory, all operations will be …\nThe root for surrealdb.\nThe root for Swift.\nroot of this backend.\nroot of this backend.\nroot of this backend\nRoot for webhdfs.\nroot of this backend.\nSet the runtime token for ghac service.\nThe runtime token for ghac service.\nSet sas_token of this backend.\nSet the sas_token of this backend.\nThe sas token of Azblob service backend.\nsas_token The shared access signature token.\nThe sas token for azfile.\nset the GCS service scope\nScope for gcs.\nSet secret_access_key of this backend.\nSet secret_access_key of this backend.\nSecret access key for obs.\nsecret_access_key of this backend.\nSet secret_id of this backend.\nSecret ID of this backend.\nSet secret_key of this backend.\nSecret key of this backend.\nSet temporary credential used in AWS S3 connections\nSet the name of the persy segment. Will create if not …\nThat name of the persy segment.\nSet server_side_encryption for this backend.\nSet server_side_encryption for this backend.\nServer side encryption for oss.\nserver_side_encryption for this backend.\nSet server_side_encryption_aws_kms_key_id for this backend\nserver_side_encryption_aws_kms_key_id for this backend\nSet server_side_encryption_customer_algorithm for this …\nserver_side_encryption_customer_algorithm for this backend.\nSet server_side_encryption_customer_key for this backend.\nserver_side_encryption_customer_key for this backend.\nSet server_side_encryption_customer_key_md5 for this …\nSet server_side_encryption_customer_key_md5 for this …\nSet server_side_encryption_key_id for this backend.\nServer side encryption key id for oss.\nEnable server side encryption with aws managed kms key\nEnable server side encryption with customer key.\nEnable server side encryption with customer key.\nEnable server side encryption with customer managed kms key\nEnable server side encryption with s3 managed key\nSet the GCS service account.\nService Account for gcs.\nSet temporary credential used in AWS S3 connections\nsession_token (aka, security token) of this backend.\nSet file share name of this backend.\nThe share name for azfile.\nSet sts_endpoint for this backend.\n<code>sts_endpoint</code> will be loaded from\nSet the table name of the d1 service to read/write.\nSet the table name of the mysql service to read/write.\nSet the table name of the postgresql service to read/write.\nSet the table name for Redb. Will create if not exists.\nSet the table name of the sqlite service to read/write.\nSet the table name of the surrealdb service for read/write.\nSet the table of D1 Database.\nThe table name for mysql.\nthe table of postgresql\nThe table name for redb.\nSet the table name of the sqlite service to read/write.\nThe table for surrealdb.\nSet tenant_id of this backend.\ntenant_id The tenant id of the service principal.\nSets the time to idle of the cache.\nSets the time to idle of the cache.\nSets the time to idle of the cache.\nSets the time to idle of the cache.\nSets the time to live of the cache.\nSets the time to live of the cache.\nSets the time to live of the cache.\nSets the time to live of the cache.\nSet the token used to authenticate with CloudFlare.\nSet api token for the cloudflare d1 service.\nSet the token of this backend.\nProvide the OAuth2 token to use.\nGithub access_token.\nset bearer token for http backend\nSet the token of this backend.\nSet the token of this backend.\nVercel Blob token.\nset the bearer token for Webdav\nThe token used to authenticate with CloudFlare.\nSet the token of cloudflare api.\nThe token for dbfs.\nA Google Cloud OAuth2 token.\nGitHub access_token.\ntoken of this backend\nToken of this backend.\nThe token for Swift.\nvercel blob token.\ntoken of this backend\nSet the tree for sled.\nThe tree for sled.\nset user for ftp backend.\nSet user of this backend\nset user for sftp backend.\nuser of this backend\nuser of this backend\nuser of this backend\nSet the username of this backend, used for authentication\nName of the user for webhdfs.\nset the username for etcd\nset username for http backend\nSet username of this backend. This is required.\nset the username.\nPcloud username.\nset the username for redis\nusername of this backend.\nSet the username of the surrealdb service for signin.\nset the username for Webdav\nthe username to connect etcd service.\nusername of this backend\nUsername for Lakefs basic authentication.\nMemcached username, optional.\npCloud username.\nthe username to connect redis service.\nusername of this backend.\nThe username for surrealdb.\nusername of this backend\nSet the value field name of the d1 service to read/write.\nSet the value field name of the MongoDB service to …\nSet the value field name of the mysql service to …\nSet the value field name of the postgresql service to …\nSet the value field name of the sqlite service to …\nSet the value field name of the surrealdb service for …\nSet the value field of D1 Database.\nvalue field of this backend\nThe value field name for mysql.\nthe value field of postgresql\nSet the value field name of the sqlite service to …\nThe value field for surrealdb.\nset the version that used by cache.\nThe version that used by cache.")